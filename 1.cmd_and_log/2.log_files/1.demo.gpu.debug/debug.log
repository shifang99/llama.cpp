llama.cpp# gdb --args  ./main -m ./models/chinese-llama-2-7b/ggml-model-f16.gguf -n 64 --n_gpu_layers 100 --prompt "很久很久以前，有一个"
GNU gdb (Ubuntu 12.1-0ubuntu1~22.04) 12.1
Copyright (C) 2022 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type "show copying" and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<https://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from ./main...
(gdb) b examples/main/main.cpp:127
Breakpoint 1 at 0x217e7f: file examples/main/main.cpp, line 127.
(gdb) run
Starting program: /home/sse-ard/shifang.xu/a.dir_for_v100/4.cuda/3.llama.cpp/backup_2.llama.cpp_build/llama.cpp/main -m ./models/chinese-llama-2-7b/ggml-model-f16.gguf -n 64 --n_gpu_layers 100 --prompt 很久很久以前，有一个
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
warning: not compiled with GPU offload support, --n-gpu-layers option will be ignored
warning: see main README.md for information on enabling GPU BLAS support

Breakpoint 1, main (argc=9, argv=0x7fffffffda88) at examples/main/main.cpp:127
127         llama_sampling_params & sparams = params.sparams;
(gdb) p parames
No symbol "parames" in current context.
(gdb) p params
$1 = {seed = 4294967295, n_threads = 36, n_threads_draft = -1, n_threads_batch = -1, n_threads_batch_draft = -1,
  n_predict = 64, n_ctx = 512, n_batch = 2048, n_ubatch = 512, n_keep = 0, n_draft = 5, n_chunks = -1, n_parallel = 1,
  n_sequences = 1, p_split = 0.100000001, n_gpu_layers = 100, n_gpu_layers_draft = -1, split_mode = LLAMA_SPLIT_MODE_LAYER,
  main_gpu = 0, tensor_split = {0 <repeats 128 times>}, n_beams = 0, grp_attn_n = 1, grp_attn_w = 512, n_print = -1,
  rope_freq_base = 0, rope_freq_scale = 0, yarn_ext_factor = -1, yarn_attn_factor = 1, yarn_beta_fast = 32,
  yarn_beta_slow = 1, yarn_orig_ctx = 0, defrag_thold = -1, cb_eval = 0x0, cb_eval_user_data = 0x0,
  numa = GGML_NUMA_STRATEGY_DISABLED, rope_scaling_type = LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
  pooling_type = LLAMA_POOLING_TYPE_UNSPECIFIED, sparams = {n_prev = 64, n_probs = 0, min_keep = 0, top_k = 40,
    top_p = 0.949999988, min_p = 0.0500000007, tfs_z = 1, typical_p = 1, temp = 0.800000012, dynatemp_range = 0,
    dynatemp_exponent = 1, penalty_last_n = 64, penalty_repeat = 1, penalty_freq = 0, penalty_present = 0, mirostat = 0,
    mirostat_tau = 5, mirostat_eta = 0.100000001, penalize_nl = false, seed = 4294967295,
    samplers_sequence = std::vector of length 6, capacity 6 = {llama_sampler_type::TOP_K, llama_sampler_type::TFS_Z,
      llama_sampler_type::TYPICAL_P, llama_sampler_type::TOP_P, llama_sampler_type::MIN_P, llama_sampler_type::TEMPERATURE},
    grammar = "", cfg_negative_prompt = "", cfg_scale = 1, logit_bias = std::unordered_map with 0 elements,
    penalty_prompt_tokens = std::vector of length 0, capacity 0, use_penalty_prompt_tokens = false},
  model = "./models/chinese-llama-2-7b/ggml-model-f16.gguf", model_draft = "", model_alias = "unknown", model_url = "",
  hf_repo = "", hf_file = "", prompt = "很久很久以前，有一个", prompt_file = "", path_prompt_cache = "",
  input_prefix = "", input_suffix = "", antiprompt = std::vector of length 0, capacity 0, logdir = "",
  lookup_cache_static = "", lookup_cache_dynamic = "", logits_file = "", kv_overrides = std::vector of length 0, capacity 0,
  lora_adapter = std::vector of length 0, capacity 0, lora_base = "", control_vectors = std::vector of length 0, capacity 0,
  control_vector_layer_start = -1, control_vector_layer_end = -1, ppl_stride = 0, ppl_output_type = 0, hellaswag = false,
  hellaswag_tasks = 400, winogrande = false, winogrande_tasks = 0, multiple_choice = false, multiple_choice_tasks = 0,
  kl_divergence = false, random_prompt = false, use_color = false, interactive = false, chatml = false,
  prompt_cache_all = false, prompt_cache_ro = false, embedding = false, escape = false, interactive_first = false,
  multiline_input = false, simple_io = false, cont_batching = true, input_prefix_bos = false, ignore_eos = false,
  instruct = false, logits_all = false, use_mmap = true, use_mlock = false, verbose_prompt = false, display_prompt = true,
  infill = false, dump_kv_cache = false, no_kv_offload = false, warmup = true, cache_type_k = "f16", cache_type_v = "f16",
  mmproj = "", image = ""}
(gdb)

